# MoE-Ascend-Opt: åä¸º NPU ä¸Šçš„é«˜æ€§èƒ½ AWQ MoE æ¨ç†ä¼˜åŒ–

![Platform](https://img.shields.io/badge/Platform-Huawei%20Ascend%20910B-red)
![Model](https://img.shields.io/badge/Model-Qwen3%20MoE-blue)
![Quantization](https://img.shields.io/badge/Quantization-AWQ%20Int4-green)

**MoE-Ascend-Opt** æ˜¯ä¸€ä¸ªè‡´åŠ›äºåœ¨åä¸º Ascend 910B NPU ä¸ŠåŠ é€Ÿæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰ï¼Œç‰¹åˆ«æ˜¯ Qwen2.5/3-MoE ç³»åˆ— AWQ (W4A16) é‡åŒ–æ¨¡å‹çš„å¼€æºä¼˜åŒ–é¡¹ç›®ã€‚

æœ¬é¡¹ç›®åŸºäº [SGLang](https://github.com/sgl-project/sglang) æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è‡ªå®šä¹‰çš„ Ascend C ç®—å­å’Œ Triton kernelï¼Œè§£å†³äº†åŸç”Ÿå®ç°ä¸­å° Batch Size ä¸‹çš„æ˜¾å­˜å¸¦å®½ç“¶é¢ˆé—®é¢˜ã€‚åœ¨ Decoding é˜¶æ®µï¼Œç«¯åˆ°ç«¯ååé‡å®ç°äº† **~2å€** çš„æå‡ã€‚

---

## ğŸš€ æ€§èƒ½è¡¨ç° (Performance)

**æµ‹è¯•æ¨¡å‹**: `tclf90/Qwen3-30B-A3B-Thinking-2507-AWQ`
**ç¡¬ä»¶ç¯å¢ƒ**: Huawei Ascend 910B

### 1. Kernel å¾®åŸºå‡†æµ‹è¯• (Fused MoE Layer)

å¯¹æ¯”åŸç”Ÿ PyTorch/Ascend å®ç° (Ref) ä¸æœ¬é¡¹ç›®ä¼˜åŒ–åçš„ Kernel (Custom)ã€‚

| Batch Size | Ref Latency (us) | Custom Latency (us) | Ref Bandwidth (GB/s) | Custom Bandwidth (GB/s) | **åŠ é€Ÿæ¯” (Speedup)** |
| :---: | :---: | :---: | :---: | :---: | :---: |
| **1** | 207.61 | **52.99** | 96.59 | 378.45 | **3.92x** |
| **2** | 279.09 | **72.59** | 143.71 | 552.53 | **3.84x** |
| **3** | 408.59 | **93.67** | 147.24 | 642.30 | **4.36x** |
| **4** | 519.75 | **117.09** | 154.34 | 685.05 | **4.44x** |

### 2. SGLang ç«¯åˆ°ç«¯ Decoding ååé‡
è®¾ç½®ï¼šInput = 1 token, Output = 1024 tokensã€‚

| Batch Size | åŸç”Ÿååé‡ (tokens/s) | ä¼˜åŒ–åååé‡ (tokens/s) | **æå‡å€æ•° (Speedup)** |
| :---: | :---: | :---: | :---: |
| 1 | 52.99 | 91.92 | **1.73x** |
| 2 | 83.86 | 163.43 | **1.95x** |
| 4 | 127.58 | 272.99 | **2.14x** |
| 6 | 155.48 | 340.32 | **2.19x** |
| 8 | 181.69 | 403.48 | **2.22x** |

---

## ğŸ§  æ ¸å¿ƒä¼˜åŒ–åŸç† (Optimization Principles)

### 1. ç—›ç‚¹ï¼šæ˜¾å­˜å¸¦å®½ç“¶é¢ˆ
åœ¨ Ascend NPU çš„åŸç”Ÿ AWQ å®ç°ä¸­ï¼Œè®¡ç®—æµç¨‹é€šå¸¸æ˜¯ï¼š
1.  **åé‡åŒ–**ï¼šå°† Int4 æƒé‡åŠ è½½å¹¶è½¬æ¢ä¸º FP16ï¼Œç»“æœ**å†™å›**å…¨å±€å†…å­˜ (Global Memory, GM)ã€‚
2.  **çŸ©é˜µä¹˜**ï¼šCube Core ä» GM è¯»å– FP16 æƒé‡è¿›è¡ŒçŸ©é˜µä¹˜æ³•ã€‚

åœ¨å° Batch Sizeï¼ˆDecoding é˜¶æ®µï¼‰ä¸‹ï¼Œè¿™ç§ Read-Write-Read çš„æ¨¡å¼æå¤§åœ°æµªè´¹äº† NPU çš„æ˜¾å­˜å¸¦å®½ï¼Œå¯¼è‡´è®¡ç®—å—é™äºå¸¦å®½è€Œéç®—åŠ›ã€‚

### 2. è§£å†³æ–¹æ¡ˆï¼šåŸºäº Vector Core çš„ W4A16 GEMV
æˆ‘ä»¬ä½¿ç”¨äº† Ascend C ç¼–å†™äº†è‡ªå®šä¹‰ç®—å­ï¼Œåˆ©ç”¨ **Vector Core (AI Vector)** æ›¿ä»£ Cube Core å¤„ç†å° Batch åœºæ™¯ï¼š
*   **å¯„å­˜å™¨çº§åé‡åŒ–**ï¼šæƒé‡ä»¥ Int4 å½¢å¼ä» GM åŠ è½½åˆ°ç‰‡ä¸Šå†…å­˜ (UB)ï¼Œç›´æ¥åœ¨ Vector å•å…ƒå¯„å­˜å™¨ä¸­å®Œæˆåé‡åŒ–ã€‚
*   **æ¶ˆé™¤ä¸­é—´è¯»å†™**ï¼šåé‡åŒ–åçš„ FP16 æ•°æ®ç›´æ¥å‚ä¸ç‚¹ç§¯è®¡ç®—ï¼Œæ— éœ€å†™å› GMã€‚
*   **ç»“æœ**ï¼šå¤§å¹…å‡å°‘äº†å¯¹ GM çš„è®¿é—®æ¬¡æ•°ï¼Œæ˜¾è‘—æå‡äº†å¸¦å®½åˆ©ç”¨ç‡ã€‚

### 3. å‚ç›´ç®—å­èåˆ (Vertical Fusion)
ä¸ºäº†è¿›ä¸€æ­¥å‡å°‘ Kernel Launch å¼€é”€å’Œæ•°æ®æ¬è¿ï¼Œæˆ‘ä»¬å¯¹ MoE çš„ MLP å—è¿›è¡Œäº†ç®€æ˜“çš„å‚ç›´èåˆï¼š
*   **åŸç”Ÿæµç¨‹**ï¼š`GEMM(Gate)` -> GM -> `GEMM(Up)` -> GM -> `SwiGLU` -> GM -> `GEMM(Down)`ã€‚
*   **èåˆæµç¨‹**ï¼šä¸€ä¸ª Kernel å®Œæˆæ‰€æœ‰æ“ä½œã€‚
    *   åŠ è½½è¾“å…¥ Xã€‚
    *   è®¡ç®— Gate å’Œ Up æŠ•å½±ã€‚
    *   åœ¨ç‰‡ä¸Šå¿«é€Ÿè¿›è¡Œ SwiGLU æ¿€æ´»ã€‚
    *   è®¡ç®— Down æŠ•å½±å¹¶ç´¯åŠ ç»“æœã€‚

### 4. Triton åœ¨ Ascend ä¸Šçš„å°è¯•
æœ¬é¡¹ç›®è¿˜æ¢ç´¢äº† OpenAI Triton åœ¨ NPU ä¸Šçš„åº”ç”¨ï¼Œç”¨äºå¤„ç†éè®¡ç®—å¯†é›†å‹ä½†é€»è¾‘å¤æ‚çš„æ“ä½œï¼Œæé«˜å¼€å‘æ•ˆç‡ï¼š
*   **MoE Gating (TopK Softmax)**: ä½¿ç”¨ Triton å®ç°äº† Router Logits çš„ Softmax å’Œ TopK é€‰æ‹©ï¼Œé¿å…äº†æ‰‹å†™å¤æ‚çš„ C++ Tiling é€»è¾‘ã€‚
*   **Weight Repacking**: ä½¿ç”¨ Triton å®ç°äº†æƒé‡çš„ Layout è½¬æ¢ (`sgl_kernel_npu/repack_int4.py`)ï¼Œå°†é€šç”¨ AWQ æƒé‡é‡æ’ä¸º NPU Vector æŒ‡ä»¤æ‰€éœ€çš„æ ¼å¼ã€‚

---

## ğŸ“‚ é¡¹ç›®ç»“æ„

```text
MoE-Ascend-Opt
â”œâ”€â”€ sglang/                 # ä¿®æ”¹åçš„ SGLang æ¡†æ¶æºç 
â”‚   â”œâ”€â”€ python/sglang/srt/layers/moe/topk.py      # Triton å®ç°çš„ Gating é€»è¾‘
â”‚   â”œâ”€â”€ python/sglang/srt/layers/quantization/awq.py # é€‚é… NPU ä¼˜åŒ–çš„ AWQ é€»è¾‘
â”‚   â””â”€â”€ ...
â”œâ”€â”€ sgl-kernel-npu/         # è‡ªå®šä¹‰ NPU ç®—å­åº“
â”‚   â”œâ”€â”€ csrc/grouped_gemv/  # æ ¸å¿ƒ Ascend C ä»£ç  (W4A16 GEMV & Fused MoE)
â”‚   â”œâ”€â”€ python/             # Python ç»‘å®šä¸ Triton Kernel
â”‚   â”œâ”€â”€ build.sh            # ç¼–è¯‘è„šæœ¬
â”‚   â””â”€â”€ ...
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ å®‰è£…ä¸æ„å»º (Installation)

### ç¯å¢ƒè¦æ±‚
*   **Hardware**: Huawei Ascend 910B
*   **Software**: CANN Toolkit 8.0+, PyTorch (Ascend version)

### æ­¥éª¤ 1: ç¼–è¯‘è‡ªå®šä¹‰ç®—å­åº“
ç¼–è¯‘åŒ…å« Ascend C kernel çš„ `sgl-kernel-npu` æ‰©å±•ã€‚

```bash
cd sgl-kernel-npu
# å¯é€‰ï¼šæ¸…ç†æ—§çš„æ„å»º
./build.sh -c 

# ç¼–è¯‘å¹¶å®‰è£… whl åŒ…
./build.sh
pip install dist/sgl_kernel_npu-*.whl
```

### æ­¥éª¤ 2: å®‰è£…ä¿®æ”¹ç‰ˆ SGLang
å®‰è£…é›†æˆäº†ä¸Šè¿°ç®—å­è°ƒç”¨çš„ SGLang æ¡†æ¶ã€‚

```bash
cd ../sglang/python
pip install -e .
```

---

## ğŸš€ å¯åŠ¨æœåŠ¡ (Usage)

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ SGLang Serverã€‚

**å…³é”®å‚æ•°è¯´æ˜**:
*   `--sampling-backend ascend`: **å¿…é¡»å¼€å¯**ã€‚å¯ç”¨é’ˆå¯¹ Ascend NPU ä¼˜åŒ–çš„é‡‡æ ·åç«¯ã€‚
*   `--dtype float16`: **å¿…é¡»è®¾ç½®**ã€‚Ascend NPU ä¸Š float16 æ€§èƒ½é€šå¸¸ä¼˜äº bfloat16ï¼Œä¸”é€‚é…å½“å‰ç®—å­å®ç°ã€‚
*   `--cuda-graph-bs`: è®¾ç½®æ•è· CUDA Graph çš„ Batch Size åˆ—è¡¨ï¼Œå»ºè®®è¦†ç›–å¸¸ç”¨çš„ Decoding Batch å¤§å°ã€‚

**å¯åŠ¨å‘½ä»¤ç¤ºä¾‹**:

```bash
python3 -m sglang.launch_server \
  --model ~/data/models/Qwen3-30B-A3B-Thinking-2507-AWQ \
  --attention-backend ascend \
  --mem-fraction-static 0.9 \
  --reasoning-parser qwen3-thinking \
  --tp-size 1 \
  --sampling-backend ascend \
  --cuda-graph-bs 1 2 3 4 5 6 7 8 \
  --dtype float16 \
  --chunked-prefill-size 4096
```

---

## âš ï¸ é™åˆ¶è¯´æ˜ (Limitations)

1.  **Batch Size**: è‡ªå®šä¹‰çš„ Vector-based GEMV ä¸»è¦é’ˆå¯¹ **BS <= 8** çš„ Decoding é˜¶æ®µä¼˜åŒ–ã€‚å¯¹äºå¤§ Batch (Prefill é˜¶æ®µ)ï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨å›é€€åˆ°åŸç”Ÿçš„ Cube-based çŸ©é˜µä¹˜æ³•ä»¥ä¿è¯ååé‡ã€‚
2.  **æ¨¡å‹æ”¯æŒ**: ç›®å‰ä¸»è¦åœ¨ **Qwen3-MoE** æ¶æ„ + **AWQ** é‡åŒ–ä¸‹è¿›è¡Œäº†éªŒè¯ã€‚
3.  **ç¡¬ä»¶ä¾èµ–**: ä»£ç é’ˆå¯¹ Ascend 910B (AIV æ¶æ„) è¿›è¡Œäº†æŒ‡ä»¤çº§ä¼˜åŒ–ï¼Œæ— æ³•ç›´æ¥åœ¨æ—§ç‰ˆ NPU æˆ– GPU ä¸Šè¿è¡Œã€‚

---
